{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecção de Mudança\n",
    "\n",
    "## Bibliotecas\n",
    "\n",
    "### Pytorch\n",
    "\n",
    "[Pytorch](https://pytorch.org/docs/stable/index.html) é uma biblioteca otimizada de *tensors* para *Deep Learning* que utiliza GPUs (*Central Processing Unit*) e CPUs (*Graphics Processing Unit*) - [diferença entre CPUs e GPUs](https://canaltech.com.br/hardware/o-que-e-cpu-gpu-diferencas-154939/).\n",
    "\n",
    "[Pytorch](https://en.wikipedia.org/wiki/PyTorch) é uma biblioteca de *Machine Learning* baseada em *Torch*, é utilizada em aplicações de visão computacional e processamento de linguagem natural, primeiramente desenvolvida pelo *Facebook's AI Research lab (FAIR)*. Vários *softwares* que utilizam *Deep Learning* são desenvolvidos com base em *Pytorch*, como por exemplo Tesla Autopilot, Uber's Pyro, uggingFace's Transformers, yTorch Lightning, and Catalyst.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch imported - Ok\n"
     ]
    }
   ],
   "source": [
    "# Pytorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision.transforms as tr # transforms, data augmentation\n",
    "#...\n",
    "print(\"Pytorch imported - Ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models imported - Ok\n"
     ]
    }
   ],
   "source": [
    "# Models\n",
    "import import_ipynb\n",
    "\n",
    "from unet import unet\n",
    "%run ../models/f-res-unet.ipynb # import F_res_UNet\n",
    "#from ../models/f-res-unet.ipynb import F_res_UNet\n",
    "#import models/f-res-unet.ipynb\n",
    "\n",
    "print(\"Models imported - Ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTE 2\n"
     ]
    }
   ],
   "source": [
    "%run test.ipynb import Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class ChangeDetectionDataset\n",
      "Dataset imported - Ok\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "\n",
    "# import functions from dataset/\n",
    "%run ../dataset/dataset-utl.ipynb\n",
    "\n",
    "print(\"Dataset imported - Ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Others imported - Ok\n"
     ]
    }
   ],
   "source": [
    "# Other\n",
    "\n",
    "import nbimporter # help imports between notebooks\n",
    "import os # operational system\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pandas import read_csv\n",
    "from math import floor, ceil, sqrt, exp\n",
    "import time\n",
    "from scipy.ndimage import zoom # zoom image\n",
    "from tqdm import tqdm as tqdm # \"progress\" (taqadum, تقدّم) - te quiero demasiado\n",
    "from skimage import io # skimage.io.imread(fname[, as_gray, plugin]) - Load an image from file\n",
    "# ...\n",
    "\n",
    "print(\"Others imported - Ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definições Globais\n",
    "\n",
    "- **IS_LOCAL**: **True** - rodando programa localmente - sem acesso a GPU. **False** - rodando no Google Colab com acesso a GPU;\n",
    "- **IS_COLAB**: **True** - utiliza o Google Colab\n",
    "\n",
    "Alterações são necessárias na estrutura do código ao utilizar o [Google Colab](https://colab.research.google.com/). O Dataset estará no Drive. \n",
    "\n",
    "- ATENÇÃO: **TODO**: aprender a salvar arquivos no Google Drive quando a aplicação estiver utilizando o Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Colab setup - Ok\n"
     ]
    }
   ],
   "source": [
    "# initial definitions/imports for Google Colab\n",
    "\n",
    "IS_LOCAL = True\n",
    "IS_COLAB = False\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"Using Google Colab...\")\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/dataset')\n",
    "    \n",
    "print(\"Google Colab setup - Ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DATASET_PATH:\n",
    "- IS_PROTOTYPE:\n",
    "- BATCH_SIZE: \n",
    "- PATCH_SIDE:\n",
    "- N_EPOCHS:\n",
    "- NORMALISE_IMGS:\n",
    "- TRAIN_STRIDE:\n",
    "- BAND_TYPE:\n",
    "- LOAD_TRAINED:\n",
    "- DATA_AUG:\n",
    "- IS_LOCAL: **True** - rodando programa localmente - sem acesso a GPU. **False** - rodando no Google Colab com acesso a GPU;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global definitions - Ok\n"
     ]
    }
   ],
   "source": [
    "# Global variables definition\n",
    "\n",
    "# IS_PROTOTYPE = False\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "PATCH_SIDE = 96\n",
    "# N_EPOCHS = 50\n",
    "N_EPOCHS = 5 # hard to use large number of epochs even in colab\n",
    "\n",
    "# NORMALISE_IMGS = True\n",
    "\n",
    "TRAIN_STRIDE = int(PATCH_SIDE/2) - 1 # 47\n",
    "\n",
    "# ??\n",
    "BAND_TYPE = 4 # 0-RGB | 1-RGBIr | 2-All bands s.t. resulution <= 20m | 3-All bands\n",
    "\n",
    "# LOAD_TRAINED = False\n",
    "\n",
    "DATA_AUGMENTATION = True\n",
    "\n",
    "FP_MODIFIER = 10 # tunning parameter, use 1 if unsure\n",
    "\n",
    "DATASET_LOCAL_PATH = '/home/virginia/Github/change-detection/onera-oscd/OSCD/'\n",
    "DATASET_COLAB_PATH = '/content/dataset/MyDrive/OSCD/'\n",
    "\n",
    "if IS_COLAB:\n",
    "    DATASET_PATH = DATASET_COLAB_PATH\n",
    "else:\n",
    "    DATASET_PATH = DATASET_LOCAL_PATH\n",
    "\n",
    "print(\"Global definitions - Ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "Recentes avanços na aquisição e processamento de imagens digitais permitiram o uso eficiente e inovador de dados fornecidos por meio de bases de dados em aplicações de inteligência artificial e redes neurais (ML, DL). Entretanto, ainda existe uma deficiência na obtenção dessa base de dados, pois determinadas técnicas computacionais necessitam de um grande volume de dados para fornecer soluções confiáveis e com capacidade de generalizar diferentes entradas. É nesse contexto que se inserem as técnicas de *Data Augmentation* em imagens.\n",
    "\n",
    "Essa técnica tem como intuito gerar novos dados virtuais a partir de exemplos reais para prover mais volume de informações e dar suporte ao desenvolvimento de aplicações mais robustas.\n",
    "\n",
    "Uma das técnicas que podem ser utilizadas é são métodos de transformações geométricas, por exemplo: *flipping*, rotação, translação, *cropping* (corte), *zoom* (escala), *shear* (cisalhamento).\n",
    "\n",
    "(Data Agmentation)[https://sol.sbc.org.br/livros/index.php/sbc/catalog/download/48/217/455-1?inline=1]\n",
    "\n",
    "TO-DO:\n",
    "- analisar *overfitting*\n",
    "- analisar *oversampling*\n",
    "\n",
    "### Compositions of transforms\n",
    "\n",
    "```\n",
    "torchvision.transforms.Compose(transforms)\n",
    "```\n",
    "\n",
    "- **transforms** (list of ```Transform``` objects) – list of transforms to compose.\n",
    "\n",
    "Composes several transforms together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data augmentation - Ok\n"
     ]
    }
   ],
   "source": [
    "if DATA_AUGMENTATION:\n",
    "    data_aug_transform = tr.Compose([RandomFlip(), RandomRot()])\n",
    "else:\n",
    "    data_aug_transform = None\n",
    "\n",
    "print(\"Data augmentation - Ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset\n",
    "\n",
    "Prepare Dataset to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'adjust_shape' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-01790db60712>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChangeDetectionDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRAIN_STRIDE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_aug_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mIS_COLAB\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# don't forget to activate GPU in Google Colab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-87-9d96080e801d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, train, patch_side, stride, use_all_bands, transform)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# tqdm derives from the Arabic word taqaddum (تقدّم) which can mean \"progress,\" and is an abbreviation for \"I love you so much\" in Spanish (te quiero demasiado).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# load and store each image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mI1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_sentinel_img_trio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mimg_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreshape_torch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreshape_torch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-87-e6b6707b66b3>\u001b[0m in \u001b[0;36mread_sentinel_img_trio\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# TO-DO: test with read_sentinel_img\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# BAND_TYPE is defined globally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mI1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_sentinel_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mIMG_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mI2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_sentinel_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mIMG_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-87-bae140d17eb6>\u001b[0m in \u001b[0;36mread_sentinel_img\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mBAND_TYPE\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0muv\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0madjust_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzoom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mim_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mSENTINEL_AEROSOL_60m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mwv\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0madjust_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzoom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mim_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mSENTINEL_WATER_VAPOR_60m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mcir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madjust_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzoom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mim_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mSENTINEL_CIRRUS_60m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'adjust_shape' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset = ChangeDetectionDataset(DATASET_PATH, train=True, stride=TRAIN_STRIDE, transform=data_aug_transform)\n",
    "\n",
    "if IS_COLAB: # don't forget to activate GPU in Google Colab\n",
    "    weights = torch.FloatTensor(train_dataset.weights).cuda()\n",
    "else:\n",
    "    weights = torch.FloatTensor(train_dataset.weights)\n",
    "    \n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9973cb7371b5c2be5f18a8b8225bf4e6ad54a34fb5872a1bc006d10d97ce0f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python38564bitbasecondab918388200e74c19bbf7a29e3542111d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
